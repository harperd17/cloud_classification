# Segmentation of Cloud Classifications from Satellite Images
# Abstract
maybe keep this
# Introduction
With increasing concerns about climate change, researchers are trying to build more robust climate models. Clouds play a large role in Earth's climate, making them a crucial part of climate models. However, cloud classification is a tedious task, so automating this process would lead to great strides in existing climate models and more accurate weather prediction. In particular, the following cloud structures are of interest to scientists; Sugar, Gravel, Fish, and Flower clouds. This project is inspired from a [Kaggle competition](https://www.kaggle.com/c/understanding_cloud_organization). From looking at submissions from other Data Scientists, I saw that people took two approaches; image segmentation and bounding box prediction. My project aims to build an image segmentation model rather than a bounding box prediction model because these models are of more interest to me. For more information about the application of this project please visit the link above.
# The Data
The data used in this project is satellite imagery from NASA Worldview. The images have three bands (R, G, B) and are each accompanied with pixel encodings for four classifications (fish, flower, gravel, and sugar clouds). The pixel encodings were determined as the union of the labels produced by three different scientists per image. In total, a team of 68 scientists at the Max-Planck-Institute for Meterology in Hamburg, Germany created the labels for this data. More information can be found here. In total, there are 5,546 images with pixel encodings for the cloud classification masks. There are an additional 3,703 test images that have to pixel encoding information.
# Exploratory Data Analysis
Each image is of size 1400 X 2100 pixels and has 3 channels (RGB). The masks are mostly rigid shaped, which is strange because clouds are not rigid in shape. However, this must be because it was easier to classify the clouds by drawing a shape around them rather than actually classifying pixel by pixel on such large images. Another thing to note is that the masks often times overlap. I expect that these two qualities of this dataset will cause some troubles in this project. To see some sample images and masks, see the image below.
![Sample Images and Their Masks](../figures/sample_images_and_masks.png)

# Modeling
Show in sections what I did
### The Loss Function
Finding a good loss function was crucial for these models. Initially, I used cross entropy, but found that often times the model was simply predicting that every pixel was not a cloud. This was because for each image, there is often a blank mask for at least one of the cloud types, and for the masks that weren't blank, they usually made up a small section of the entire image. Therefore, the model could receive decent accuracy by simply saying each pixel was not any of the four classes of clouds.
Through research I found that a "Dice Loss" was the solution to this issue. A dice loss multiplies the predicted probabilities from the model with the binary labels for the respective class and multiplies the grand sum by 2. This becomes the numerator in the loss. Then the square of the predictions and mask values are summed and added together. This becomes the denominator of the loss. Since a perfect prediction would yield a score of 1 and the worst prediction would yield a score of 0, the loss value becomes 1 - dice loss so that minimizing the loss function will yield us better results instead of worse results (since the original dice loss would need to be maximized). For more information on the dice loss, visit this [article](https://www.jeremyjordan.me/semantic-segmentation/) and search for "Dice coefficient". The benefit of this loss function is that the pixel predictions for the pixels that contain the cloud type will be counted the most.
I used the implementation of the Dice loss using [segmentation_models_pytorch's implementation](https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/losses/dice.py). Based on the source code in the link, I am using 'multilabel' mode which expects multiple layers of binary labels (one layer per each cloud mask).
### Evaluation Metrics
The evaluation of the model is done through three different metrics. The first is the value of the dice loss function. The second is the overall accuracy of the prediction compared to the label masks. The third is the overall area under the curve (AUC) of the prediction compared to the label masks. These metrics are evaluated for each cloud mask and averaged.
### Dataset Abstraction
The data needed to be abstracted through the creation of a class that inherits from the PyTorch Dataset class. This class is instantiated by passing in a dataframe that has a column of strings that take the form of "{image file name} {cloud label}" for each entry. There is also a column named "EncodedPixels" which is a string of space separated numbers. First, the image file name and cloud label are parsed into their own separate columns. Then, the column EncodedPixels is parsed into a list of integers, which is then converted into a list of pairs of numbers, where each pair has a starting pixel index and a length integer. For each image file name, there are 4 rows, one for each cloud classification. The mask for each cloud classification of each image file is created using the integer pairs parsed from the "EncodedPixels" column. The starting pixel location integer corresponds to the index of a flattened image array of size 1400 X 2100 pixels. Therefore, an array of length 2,940,000 filled with 0's is created and for each integer pair, the arrays values at indices (starting pixel + length) become 1, signifiying a positive classification of that cloud. For each unique image file, the image is loaded and converted to a tensor and reshaped to [3,512,512]. Then, the mask for each of the cloud classifications is computed, reshaped to [512,512]
 and stacked into a label tensor of shape [512,512,4]. 
 The value at a particular index of this data set abstraction is the input and label tensor for that particular index in a listing of unique image file names.
## Initial Model Experimentation
At first, I wanted to try out some of the basic implementations we reviewed in class. first, I tried a basic CNN where the height and width of the layers stay constant. Second, I tried a CNN with transposes to have the model work more efficiently and hopefully learn to represent the important parts of the images within a smaller space and then use that representation to project predictions up to the correct output size. Lastly, in my initial modeling attempts, I tried a UNet model architecture. 
Since there are over 5,000 images in the training set, I decided to train these initial models on a random sample of 500 of the training images to keep training time down. The intention is to find what works best and then train that model on all of the data at the end. The results from this intial modeling is shown below.
#### Basic CNN Architecture
A good starting point for this project was to do some simple modeling using CNNs with padding such that the convolutions are the same size at each layer. After playing around with the depth of the model, using a simple Adam optimizer (using the default learning rate and no scheduler), I found that after 8 convolutional layers (the first four having a kernel size of 9 and the last 4 having a kernel size of 3, and all layers have 64 filters), each followed by batch normalization and a leakly relu activation, the model didn't really see much improvement. The dice loss value leveled out at ~55%, the overall accuracy at ~75%, and the AUC at ~75%. For visualizations of this work, visit the [CNN Modeling Notebook](https://github.com/harperd17/cloud_classification/blob/main/modeling/CNN_segmentation_model.ipynb). From looking at the visualizations, the predicted segmentation is not performing well. For blank masks, the predictions yield significant areas of positive classifications. In addition, the areas of positive classification predictions are not shaped similar to the label masks and also have holes, which is to be expected with this basic architecture.
#### CNN Architecture with Transposes
The next step for me was the explore a basic CNN architecture, but with transposed layers so that the layer shapes decrease and then later increase. After experimenting with different levels of complexity, I found that using 5 rounds of pooling (factor of 2) followed by 2 convolutional layers. The convolutional layers in the first round have 64 filters, and each subsequent round has twice the ammount of filters as the previous one. This is then followed by 5 rounds of transposed convolutional layers with the opposite pattern in the number of filters. Each convolutional layer in this model has a layer of batch normalization and a leaky relu activation. Making the model more complex didn't yield substantially better results. This models dice loss flattened at ~50%, the accuracy topped out at ~80%, and the AUC topped out at ~75%. The visualizations of the predictions from this model are similar in quality to those from the basic CNN architecture, with the exception that the holes in the predicted masks have disappeared. The see the visualizations, visit the [CNN Modeling with Transposes Notebook](https://github.com/harperd17/cloud_classification/blob/main/modeling/transposed_CNN_segmentation_model.ipynb).
#### UNet Architecture
I also wanted to try designing and training a Unet model from scratch. The unet approach seems to be useful for this dataset because I would imagine cloud classification factors in both low level and high level details to make a classification for each pixel. The model I designed has 4 UNet blocks, ending with a convolutional layer that outputs 4 filters (one per class of cloud). This acheived a dice loss of ~50%, an accuracy of ~80%, and an AUC of ~78%. When looking at the predicted masks from this model, the results seem similar to when using the methods above. The predictions don't resemble the true masks very well. To see the results from this modeling, visit the [UNet Modeling Notebook](https://github.com/harperd17/cloud_classification/blob/main/modeling/UNet_segmentation_model.ipynb).
## Experimentation Using Existing Model Architectures
Next, I decided to try out an existing model architecture. I downloaded the "UnetResNet" model from the [TorchSat library](https://github.com/sshuair/torchsat) withe ** encoder depth, 4 classes, 3 input channels, *** filters, and a dropout rate of *** Since there were pre-trained weights available, I used them. I used these weights as a "warm start" by training the model on my data, allowing all the weights to be adjusted during training. In order to make the model more robust I added transformations. The first is required for the pretrained model to work well and involves normalizing the input with means of [0.485, 0.456, 0.406] and standard deviations of [0.229, 0.224, 0.225]. The other transformations were for robustness of the model and include a random shift up to 20%, random rotation up to 15 degrees, and random brightness. The reason for the random brightness is that streaks of brightness show up in many of the satellite images, and I image this could be confusing for the model, so adding some randomly to the input images will force the model to learn to ignore it. I obtained the following results from training this model 
## Experimentation Using Thresholded Masks During Training
Since one of the limitations I had anticipated was that the cloud masks aren't exact, I decided to use adaptive thresholding on the original image in order to determine which pixels are clouds and which aren't. Then, I multiplied this thresholding mask with the classification masks so that only the positively classified pixels of the cloud mask that are actually clouds are used in the label mask. To see what this looks like and what was done, visit the [Cloud Thresholding Notebook](https://github.com/harperd17/cloud_classification/blob/main/EDA/only_clouds.ipynb).
Then, I trained the $$$$$$$$ model using these masks and acheived the following results.
## Conclusion
Talk about the best model I was able to get
## References/Citations
Posts, libraries, etc.
https://www.jeremyjordan.me/semantic-segmentation/ - i used this for explaining the dice loss.
https://github.com/qubvel/segmentation_models.pytorch - I used this for loss functions, and transforms
https://github.com/sshuair/torchsat preexisting model architectures, pretrained models,
